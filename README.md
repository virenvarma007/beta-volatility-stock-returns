Analysis
   
From the plots, it is evident that retail sales exhibit notable fluctuations at the same time as major spikes in the percentage of jumps. The correlation between these two variables suggests that changes in retail sales may serve as a precursor to increased market volatility or jumps. Specifically, periods of sharp increases or decreases in retail sales seem to coincide with significant fluctuations in the percentage of jumps, indicating that retail activity might be influencing or reacting to broader market conditions that trigger jumps.
On the other hand, consumer sentiment shows more subtle fluctuations, with some instances of decreasing sentiment aligning with increases in the percentage of jumps. However, the relationship is not consistent throughout the period, indicating that while drops in consumer sentiment might lead to heightened market volatility, this factor alone is not sufficient to predict jumps reliably.
Finally, job openings generally show a steady increase over time, with some correlation in periods where job openings accelerate rapidly. However, like consumer sentiment, job openings seem less directly tied to jumps, as increases in job openings are often more gradual and do not always correspond with sharp fluctuations in jumps. Nonetheless, during certain periods, such as post-2020, when job openings saw rapid changes, they appeared to coincide with higher percentages of jumps, indicating that labor market shifts could contribute to heightened market volatility in specific contexts.
Overall, retail sales show the strongest potential as a precursor for jumps, with consumer sentiment and job openings providing additional, but less consistent, insights.
Model - 1: Logistic Regression:
 
 Standard Logistic Regression (left plot): The ROC curve achieves an AUC of 0.61, indicating moderate performance. The curve does not sharply increase in the beginning, which means the model struggles to correctly classify true positives at lower false positive rates.
Rolling Window Predictions (middle plot): Here, the model's performance decreases slightly, with an AUC of 0.59. Rolling window estimation seems to lead to some loss in the model’s ability to distinguish between jumps and non-jumps, likely due to more variability introduced by updating the training set each year.
Fixed Window Predictions (right plot): The ROC curve for fixed window predictions has an AUC of 0.61, matching the performance of the standard logistic regression model. This indicates that while the rolling window method introduces some degradation in performance, fixed window prediction preserves the model’s effectiveness over time.
Model 2 and 3: LASSO Logistic regression and Ridge Logistic regression
 The ROC curves for the LASSO Logistic Regression (left) and Ridge Logistic Regression (right) show a significant improvement in the model’s predictive performance when compared to the earlier logistic regression models. The LASSO model achieves a near-perfect AUC of 0.9995, indicating that it has almost perfect discriminatory power, meaning it is extremely effective in distinguishing between the classes (jumps vs. non-jumps). This is

likely due to the regularization introduced by the LASSO, which helps avoid overfitting by shrinking some of the coefficients to zero, effectively selecting only the most relevant features for the model. This selective feature inclusion improves generalization to the out-of-sample data, leading to superior performance.
Similarly, the Ridge Logistic Regression model also performs exceptionally well with an AUC of 0.9831. Ridge regression penalizes the model complexity by introducing an l2l_2l2 regularization term, which controls the magnitude of the coefficients but does not force them to zero like LASSO. This allows the model to maintain a balance between fitting the data well and avoiding overfitting. Despite not being as selective as LASSO, Ridge regression’s high AUC indicates that it is still very effective at making accurate predictions. Both regularization methods significantly outperform the earlier logistic models and demonstrate the importance of penalizing complexity to improve out-of-sample forecasting.
The comparison of misclassification rates across various models shows a clear trend in the effectiveness of different algorithms for the given classification task. LightGBM emerges as the best-performing model with the lowest misclassification rate of 0.1000, followed closely by XGBoost at 0.1100 and KNN at 0.1200. These gradient boosting-based models excel in handling complex, non-linear relationships, which likely contributed to their superior performance.
On the other hand, the neural network (ANN) has the highest misclassification rate at 0.3750, indicating it struggles more than the other models in this context. This could be due to the dataset size or feature complexity, where simpler or more interpretable models like logistic regression variants (Logistic, LASSO, and Ridge) perform reasonably well with rates ranging between 0.1300 and 0.1500. Overall, ensemble methods like LightGBM and XGBoost demonstrate superior predictive power, while the ANN’s relatively high error suggests it may need further tuning or additional data for better results.
 
